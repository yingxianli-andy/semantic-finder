- 这份方案是为你深度定制的“IJCAI 投稿作战地图”。作为博士生，你需要掌控**MDP定义（数学建模）**和**核心算法（RL）**，将繁琐的工程实现和数据可视化下放。

  ---

  ### 第一部分：（你）的核心任务 —— RL算法设计与接口定义

  你的核心工作是定义这篇论文的“灵魂”，即如何把社会学问题转化为数学优化问题。你需要完成 `Agent` 类和 `Reward` 函数的设计。

  #### 1. 核心算法架构 (Semantic-FINDER)
  你需要基于 PyTorch 实现一个改进版的 FINDER。
  *   **输入 (State):** 图 $G=(V, E)$，其中每个节点 $v$ 有特征向量 $h_v$。
      *   $h_v = [d_v, x_v, e_v]$
      *   $d_v$: 归一化的度数 (Structural info)。
      *   $x_v$: 当前观点值 (-1 到 1) (Opinion info)。
      *   $e_v$: (可选) 由 LLM 预提取的语义嵌入 (Semantic embedding)。
  *   **模型 (Encoder-Decoder):**
      *   **Encoder:** 使用 `GraphSAGE` 或 `GCN`，将图结构编码为 Embedding。
      *   **Decoder:** 一个 MLP (DQN)，输入是 Encoder 的输出，输出是 $Q(s, a)$ 值。

  #### 2. RL 接口定义 (MDP Formulation)

  你需要给实习生 A 提供以下标准接口规范，让他去填充代码：

  *   **动作空间 (Action Space):**
      *   **定义：** 离散空间，大小为 $N$ (节点数量)。
      *   **Action $a_t$:** 选择网络中的一个节点 $v_{target}$ 进行干预。
      *   **约束：** 已选过的节点不能重复选（Masking）。

  *   **状态空间 (State Space):**
      *   **定义：** 当前网络的邻接矩阵 $A$ 和节点属性矩阵 $X$（观点值）。

  *   **奖励函数 (Reward Function) —— 论文核心创新点:**
      IJCAI 喜欢这种有明确物理/社会意义的 Reward。
      我们定义目标为：**最小化网络极化度 (Polarization)**。
      
      设 $P(G_t)$ 为 $t$ 时刻的极化指标，则 Reward $r_t = P(G_t) - P(G_{t+1})$ (即极化度的下降量)。
      
      **你可以尝试以下三种 Reward 的变体（写在论文里做消融实验）：**
      1.  **方差 (Variance):** 最简单。$P(G) = \text{Var}(\mathbf{x}) = \frac{1}{N} \sum (x_i - \bar{x})^2$。
      2.  **加权分歧 (Weighted Disagreement):** 考虑图结构的极化。$P(G) = \sum_{(i,j) \in E} w_{ij} (x_i - x_j)^2$ (Lyapunov 函数，表示邻居间吵架的程度)。
      3.  **舆论回音室指数 (Echo Chamber Score):** 利用不同阵营间的边数占比。

  #### 3. 核心代码逻辑 (伪代码)
  你需要写出主训练循环：

  ```python
  class SemanticFINDER(nn.Module):
      def __init__(self):
          super().__init__()
          self.encoder = GraphSAGE(...) # 你需要修改输入维度以包含观点值
          self.decoder = MLP(...)       # Q-network
  
      def forward(self, g, node_feats):
          # 你的核心算法逻辑
          embeddings = self.encoder(g, node_feats)
          q_values = self.decoder(embeddings)
          return q_values
  
  # 训练循环
  for episode in range(MAX_EPISODES):
      state = env.reset() # 实习生A写的环境
      loss = 0
      for step in range(BUDGET_K):
          # 1. FINDER 选择节点
          action_node = agent.select_action(state) 
          
          # 2. LLM (实习生B) 决定干预力度 (模拟时可以用随机数或简单启发式加速训练，测试时才挂载LLM)
          intervention_weight = llm_controller.get_weight(state, action_node)
          
          # 3. 环境演化 (实习生A)
          next_state, reward, done = env.step(action_node, intervention_weight)
          
          # 4. 存储到 Replay Buffer 并训练
          agent.train(state, action_node, reward, next_state)
  ```

  ---

  ### 第二部分：实习生 A 的任务 (Environment & Data)

  **人员画像：** 强代码能力，无学术背景。
  **管理策略：** 不要讲“MDP”、“马尔可夫”、“随机过程”。直接讲“写一个 Python Class，输入是 A，输出是 B”。

  **任务清单：**

  1.  **任务一：数据加载器 (Data Loader)**
      *   **目标：** 让代码能跑通 SNAP 数据集。
      *   **具体指令：**
          *   使用 `networkx` 库。
          *   编写函数 `load_graph(dataset_name)`。
          *   支持读取 `.edgelist` 或 `.txt` 文件（从 SNAP 下载）。
          *   **初始化：** 给每个节点随机赋值 `opinion` 属性（范围 -1 到 1）。初始化要呈现“双峰分布”（一半人是 -0.8，一半人是 0.8，模拟对立）。

  2.  **任务二：编写 Gym 环境类 (The Simulator)**
      *   **目标：** 实现 `OpinionDynamicsEnv` 类。
      *   **具体指令：**
          *   参考 `gymnasium` 的标准写法。
          *   实现 `step(action_node, weight)` 函数：
              1.  **干预：** 将 `action_node` 的观点值强制修正。公式：$x_{new} = x_{old} \times (1 - weight) + 0 \times weight$ (假设我们将他拉向中立 0)。
              2.  **演化 (关键)：** 实现 **DeGroot 模型** 或 **Friedkin-Johnsen 模型** 的矩阵运算形式。
                  *   公式：$\mathbf{x}(t+1) = \mathbf{A}_{norm} \mathbf{x}(t)$。
                  *   这里 $\mathbf{A}_{norm}$ 是行归一化的邻接矩阵。
                  *   让图里的观点流动 3-5 轮（模拟舆论发酵）。
              3.  **返回：** 返回新的图对象、奖励值（调用你写的奖励函数）、是否结束。

  3.  **任务三：性能优化**
      *   因为图可能很大（几万节点），告诉他尽量用 `scipy.sparse` 稀疏矩阵进行计算，不要用双重 for 循环。

  **给实习生 A 的参考资料：**
  *   **库文档：** [NetworkX Documentation](https://networkx.org/), [Gymnasium Documentation](https://gymnasium.farama.org/)
  *   **公式参考：** 搜索 "DeGroot Model Matrix Form" (直接给他看 Wikipedia 上的矩阵公式即可)。

  ---

  ### 第三部分：实习生 B 的任务 (LLM Interface & Visualization)

  **人员画像：** 有 LLM 背景。
  **管理策略：** 让他专注于 Prompt Engineering 和 视觉呈现，这是论文“卖相”的关键。

  **任务清单：**

  1.  **任务一：LLM 决策接口 (The Controller)**
      *   **目标：** 让 Qwen 扮演“军师”。
      *   **具体指令：**
          *   搭建 `vLLM` 或 `HuggingFace` 本地推理服务（Qwen-7B）。
          *   编写函数 `get_intervention_strategy(subgraph_text)`。
          *   **Prompt 设计 (核心)：**
              *   输入不是整张图，而是被选中节点及其邻居的文本描述。
              *   *Prompt 示例:* "Node A has an extreme opinion (-0.9). It connects to 5 neighbors with opposing views (avg 0.8). You are a mediator. Determine the 'persuasiveness score' (0.0 to 1.0) required to moderate Node A. Output only a float."
          *   **输出：** 解析 LLM 的输出，提取浮点数。

  2.  **任务二：自动化 Gephi 流水线 (Visualization)**
      *   **目标：** 生成论文 Fig 1 和 实验部分的热力图。
      *   **具体指令：**
          *   **不需要手动点鼠标。** 使用 Python 导出 `.gexf` 格式（NetworkX 自带）。
          *   **颜色映射：** 编写脚本，根据节点的 `opinion` 值 (-1 到 1) 自动分配颜色代码 (Hex Code)。例如：-1 是深红 `#FF0000`, 0 是白 `#FFFFFF`, 1 是深蓝 `#0000FF`。
          *   **布局：** 如果他会用 Gephi 的 Java 接口最好；如果不会，让他手动导入 Gephi 后，统一应用 **Force Atlas 2** 布局，并保存预设（Preset），确保所有图长得一样，只有颜色在变。

  3.  **任务三：语义增强 (Semantic Features)**
      *   如果算力允许，让他用 Qwen 预先给 SNAP 数据集里的节点生成一些“模拟推文”，然后转成 Embedding 存起来，作为你 RL 算法的输入特征之一。

  **给实习生 B 的参考资料：**
  *   **LLM 推理：** [vLLM GitHub](https://github.com/vllm-project/vllm)
  *   **可视化：** [Gephi File Format (.gexf) Primer](https://gephi.org/gexf/format/)
  *   **论文参考：** "Generative Agents: Interactive Simulacra of Human Behavior" (学习如何用 LLM 模拟行为)。

  ---

  ### 第四部分：必读参考文献 (分配给所有人)

  为了保证团队在同一个频道上，你需要把这些论文丢到群里：

  **1. 核心算法 (你和实习生 A 必读):**

  *   **FINDER 原文:** Fan, C., et al. (2020). "Finding key players in complex networks through deep reinforcement learning." *Nature Machine Intelligence*. (这是你们的圣经，复现对象).
  *   **DeGroot 模型:** DeGroot, M. H. (1974). "Reaching a consensus." *Journal of the American Statistical Association*. (了解环境是怎么演化的).

  **2. 领域背景 (你和实习生 B 必读):**
  *   **信息茧房与极化:** Barberá, P., et al. (2015). "Tweeting from left to right: Is online political communication more than an echo chamber?" *Psychological Science*.
  *   **IJCAI Social Good 风格:** 翻阅去年的 IJCAI "AI for Social Good" track 的论文，看看他们是怎么画图和讲故事的。

  **3. 数据集 (给实习生 A):**
  *   **SNAP:** Leskovec, J., & Krevl, A. (2014). {SNAP Datasets}: [stanford.edu/data](http://snap.stanford.edu/data). (特别是 Social networks 部分).

  ### 