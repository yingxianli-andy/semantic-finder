# Semantic-FINDER 项目开发文档

## 一、项目概述

本项目旨在实现 **Semantic-FINDER（语义增强的关键节点发现与干预）** 框架，用于在社交网络中通过认知干预降低群体极化度。项目基于 FINDER 论文，结合强化学习和 LLM 技术，实现"离线训练、在线应用"的模式。

### 核心目标
- 在小规模合成图上训练 RL 模型（30-50 节点）
- 将训练好的模型泛化到大规模真实网络（SNAP 数据集，数千到数万节点）
- 通过 LLM 生成干预策略，优化观点分布，降低网络极化度

---

## 二、团队成员与代号

| 角色 | 代号 | 主要职责 |
|------|------|----------|
| 博士生 | **安迪** | RL 算法设计、MDP 定义、核心算法实现、论文核心逻辑 |
| 实习生A | **哈工爷** | 环境模拟器、数据加载、观点动力学实现 |
| 实习生B | **上交爷** | LLM 接口封装、可视化、语义特征提取 |

---

## 三、项目目录结构

```
ijcai/
├── README.md                    # 项目说明
├── requirements.txt             # 依赖包列表
├── config/                      # 配置文件目录
│   ├── config.yaml             # 主配置文件
│   └── model_config.yaml       # 模型配置
├── src/                         # 源代码目录
│   ├── __init__.py
│   ├── agent/                   # 安迪：RL Agent 相关代码
│   │   ├── __init__.py
│   │   ├── semantic_finder.py  # Semantic-FINDER 主类
│   │   ├── encoder.py          # GraphSAGE/GCN 编码器
│   │   ├── decoder.py          # DQN 解码器
│   │   ├── replay_buffer.py    # 经验回放缓冲区
│   │   └── reward.py           # 奖励函数定义
│   ├── environment/             # 哈工爷：环境模拟器
│   │   ├── __init__.py
│   │   ├── opinion_env.py      # OpinionDynamicsEnv 主类
│   │   ├── dynamics.py         # DeGroot/Friedkin-Johnsen 模型
│   │   └── data_loader.py      # 数据加载器
│   ├── llm/                     # 上交爷：LLM 接口
│   │   ├── __init__.py
│   │   ├── controller.py       # LLM 控制器主类
│   │   ├── prompt_template.py  # Prompt 模板
│   │   └── semantic_feature.py # 语义特征提取
│   ├── visualization/           # 上交爷：可视化
│   │   ├── __init__.py
│   │   ├── gephi_exporter.py   # Gephi 导出工具
│   │   └── color_mapper.py     # 颜色映射工具
│   └── utils/                   # 工具函数
│       ├── __init__.py
│       ├── graph_utils.py      # 图处理工具
│       └── metrics.py          # 评估指标
├── data/                        # 数据目录
│   ├── raw/                     # 原始数据（SNAP 数据集）
│   ├── processed/               # 处理后的数据
│   └── synthetic/               # 合成图数据
├── experiments/                 # 实验脚本
│   ├── train.py                # 安迪：训练脚本
│   ├── test.py                 # 测试脚本
│   └── ablation/               # 消融实验
├── notebooks/                   # Jupyter  notebooks
│   ├── data_exploration.ipynb  # 数据探索
│   └── visualization.ipynb     # 可视化分析
├── results/                     # 实验结果
│   ├── models/                 # 保存的模型
│   ├── logs/                   # 训练日志
│   └── figures/                # 生成的图表
└── doc/                         # 文档目录
    ├── 开发文档.md              # 本文档
    ├── 任务分工.md              # 任务分工说明
    ├── 一月7号.md              # 项目方案
    └── API文档.md              # API 接口文档
```

---

## 四、详细任务分工

### 4.1 安迪（博士生）的任务

#### 4.1.1 核心职责
- **RL 算法设计与实现**
- **MDP 定义（状态空间、动作空间、奖励函数）**
- **模型架构设计（Encoder-Decoder）**
- **训练流程与超参数调优**

#### 4.1.2 具体任务清单

**任务 1：定义 MDP 接口规范**
- 文件：`src/agent/reward.py`
- 定义三种奖励函数变体：
  1. 方差（Variance）：`reward_variance(state)`
  2. 加权分歧（Weighted Disagreement）：`reward_weighted_disagreement(state)`
  3. 舆论回音室指数（Echo Chamber Score）：`reward_echo_chamber(state)`
- 接口规范：
```python
def compute_reward(state: Graph, next_state: Graph) -> float:
    """
    计算奖励值：极化度的下降量
    
    Args:
        state: 当前状态（图对象）
        next_state: 下一状态（图对象）
    
    Returns:
        reward: 奖励值（标量）
    """
    pass
```

**任务 2：实现 Semantic-FINDER 主类**
- 文件：`src/agent/semantic_finder.py`
- 类名：`SemanticFINDER`
- 继承：`nn.Module`
- 核心方法：
  - `forward(g, node_feats)` -> Q 值
  - `select_action(state, mask)` -> 动作（节点索引）
  - `update(replay_buffer, batch_size)` -> 损失值

**任务 3：实现编码器（Encoder）**
- 文件：`src/agent/encoder.py`
- 类名：`GraphEncoder`
- 使用 GraphSAGE 或 GCN
- 输入特征维度：`[d_v, x_v, e_v]`
  - `d_v`: 归一化度数
  - `x_v`: 观点值（-1 到 1）
  - `e_v`: 语义嵌入（可选，由上交爷提供）

**任务 4：实现解码器（Decoder）**
- 文件：`src/agent/decoder.py`
- 类名：`DQNDecoder`
- MLP 网络，输出 Q 值

**任务 5：实现经验回放缓冲区**
- 文件：`src/agent/replay_buffer.py`
- 类名：`ReplayBuffer`
- 存储 `(state, action, reward, next_state, done)` 元组

**任务 6：编写训练脚本**
- 文件：`experiments/train.py`
- 实现主训练循环
- 支持断点续训
- 定期保存模型检查点

**任务 7：编写测试脚本**
- 文件：`experiments/test.py`
- 在真实数据集上测试模型泛化能力
- 生成评估报告

#### 4.1.3 代码命名规范
- **文件命名**：小写字母 + 下划线，如 `semantic_finder.py`
- **类命名**：大驼峰，如 `SemanticFINDER`、`GraphEncoder`
- **函数命名**：小写字母 + 下划线，如 `compute_reward`、`select_action`
- **变量命名**：小写字母 + 下划线，如 `node_feats`、`q_values`
- **常量命名**：全大写 + 下划线，如 `MAX_EPISODES`、`BUDGET_K`

---

### 4.2 哈工爷（实习生A）的任务

#### 4.2.1 核心职责
- **环境模拟器实现**
- **数据加载与预处理**
- **观点动力学模型实现**

#### 4.2.2 具体任务清单

**任务 1：实现数据加载器**
- 文件：`src/environment/data_loader.py`
- 函数：`load_graph(dataset_name: str) -> nx.Graph`
- 功能：
  - 支持读取 `.edgelist` 或 `.txt` 文件
  - 从 SNAP 数据集下载并加载
  - 初始化节点观点值（双峰分布：一半 -0.8，一半 0.8）
- 接口规范：
```python
def load_graph(dataset_name: str, opinion_init: str = "bimodal") -> nx.Graph:
    """
    加载图数据并初始化观点值
    
    Args:
        dataset_name: 数据集名称（如 "ego-Twitter"）
        opinion_init: 观点初始化方式（"bimodal", "random", "uniform"）
    
    Returns:
        graph: NetworkX 图对象，每个节点有 'opinion' 属性
    """
    pass
```

**任务 2：实现观点动力学模型**
- 文件：`src/environment/dynamics.py`
- 类名：`OpinionDynamics`
- 实现两种模型：
  1. **DeGroot 模型**：`deGroot_update(graph, steps=3)`
  2. **Friedkin-Johnsen 模型**：`friedkin_johnsen_update(graph, alpha, steps=3)`
- 使用 `scipy.sparse` 稀疏矩阵优化性能
- 接口规范：
```python
def update_opinions(graph: nx.Graph, model: str = "degroot", 
                    steps: int = 3, alpha: float = 0.5) -> nx.Graph:
    """
    更新图中所有节点的观点值
    
    Args:
        graph: 输入图
        model: 模型类型（"degroot" 或 "friedkin_johnsen"）
        steps: 演化步数
        alpha: Friedkin-Johnsen 模型的固执度参数
    
    Returns:
        updated_graph: 更新后的图
    """
    pass
```

**任务 3：实现 Gym 环境类**
- 文件：`src/environment/opinion_env.py`
- 类名：`OpinionDynamicsEnv`
- 继承：`gymnasium.Env`
- 核心方法：
  - `reset()` -> 初始状态
  - `step(action_node, intervention_weight)` -> (next_state, reward, done, info)
  - `render()` -> 可视化（可选）
- 接口规范：
```python
class OpinionDynamicsEnv(gymnasium.Env):
    """
    观点动力学环境
    
    状态空间：图的邻接矩阵和节点观点值
    动作空间：离散空间，大小为节点数量 N
    奖励：由安迪提供的奖励函数计算
    """
    
    def __init__(self, graph: nx.Graph, budget: int, reward_fn: callable):
        """
        Args:
            graph: 初始图
            budget: 干预预算（最多选择 K 个节点）
            reward_fn: 奖励函数（由安迪提供）
        """
        pass
    
    def reset(self) -> np.ndarray:
        """重置环境，返回初始状态"""
        pass
    
    def step(self, action_node: int, intervention_weight: float) -> tuple:
        """
        执行一步动作
        
        Args:
            action_node: 被选中的节点索引
            intervention_weight: 干预权重（0-1，由上交爷的 LLM 提供）
        
        Returns:
            next_state: 下一状态
            reward: 奖励值
            done: 是否结束
            info: 额外信息
        """
        # 1. 干预：修正选中节点的观点值
        # 2. 演化：运行观点动力学模型 3-5 轮
        # 3. 计算奖励：调用 reward_fn
        # 4. 返回结果
        pass
```

**任务 4：实现合成图生成器**
- 文件：`src/environment/data_loader.py`
- 函数：`generate_synthetic_graph(n_nodes, graph_type="BA") -> nx.Graph`
- 支持生成 Barabasi-Albert (BA) 无标度网络
- 用于离线训练阶段

**任务 5：性能优化**
- 使用 `scipy.sparse` 稀疏矩阵进行矩阵运算
- 避免双重 for 循环
- 对大规模图（>10000 节点）进行性能测试

#### 4.2.3 代码命名规范
- **文件命名**：小写字母 + 下划线，如 `opinion_env.py`、`data_loader.py`
- **类命名**：大驼峰，如 `OpinionDynamicsEnv`、`OpinionDynamics`
- **函数命名**：小写字母 + 下划线，如 `load_graph`、`update_opinions`
- **变量命名**：小写字母 + 下划线，如 `graph`、`node_opinion`
- **常量命名**：全大写 + 下划线，如 `DEFAULT_STEPS`、`MAX_BUDGET`

---

### 4.3 上交爷（实习生B）的任务

#### 4.3.1 核心职责
- **LLM 接口封装**
- **可视化工具开发**
- **语义特征提取**

#### 4.3.2 具体任务清单

**任务 1：实现 LLM 控制器**
- 文件：`src/llm/controller.py`
- 类名：`LLMController`
- 使用 Qwen-7B（通过 vLLM 或 HuggingFace）
- 核心方法：
  - `get_intervention_weight(subgraph_info: dict) -> float`
  - `generate_strategy_description(node_id: int, graph: nx.Graph) -> str`
- 接口规范：
```python
class LLMController:
    """
    LLM 控制器，用于生成干预策略权重
    """
    
    def __init__(self, model_name: str = "Qwen/Qwen-7B", device: str = "cuda"):
        """
        Args:
            model_name: 模型名称或路径
            device: 设备（"cuda" 或 "cpu"）
        """
        pass
    
    def get_intervention_weight(self, node_id: int, graph: nx.Graph) -> float:
        """
        根据节点及其邻居信息，生成干预权重
        
        Args:
            node_id: 目标节点索引
            graph: 图对象
        
        Returns:
            weight: 干预权重（0.0 到 1.0 之间的浮点数）
        """
        # 1. 提取子图特征（目标节点 + 邻居）
        # 2. 生成文本描述
        # 3. 构造 Prompt
        # 4. 调用 LLM
        # 5. 解析输出，提取浮点数
        pass
```

**任务 2：实现 Prompt 模板**
- 文件：`src/llm/prompt_template.py`
- 函数：`build_intervention_prompt(subgraph_text: str) -> str`
- Prompt 示例：
```
"Node A has an extreme opinion (-0.9). It connects to 5 neighbors with 
opposing views (avg 0.8). You are a mediator. Determine the 'persuasiveness 
score' (0.0 to 1.0) required to moderate Node A. Output only a float."
```

**任务 3：实现语义特征提取**
- 文件：`src/llm/semantic_feature.py`
- 函数：`extract_semantic_embedding(node_id: int, graph: nx.Graph) -> np.ndarray`
- 功能：
  - 为每个节点生成"模拟推文"（使用 LLM）
  - 转换为 Embedding 向量
  - 存储为特征，供安迪的 RL 算法使用

**任务 4：实现 Gephi 导出工具**
- 文件：`src/visualization/gephi_exporter.py`
- 函数：`export_to_gexf(graph: nx.Graph, filepath: str, color_by_opinion: bool = True)`
- 功能：
  - 导出 `.gexf` 格式文件
  - 根据观点值自动分配颜色
  - 支持批量导出（用于生成动画效果）

**任务 5：实现颜色映射工具**
- 文件：`src/visualization/color_mapper.py`
- 函数：`map_opinion_to_color(opinion: float) -> str`
- 颜色方案：
  - -1.0（极左）→ `#FF0000`（深红）
  - 0.0（中立）→ `#FFFFFF`（白色）
  - 1.0（极右）→ `#0000FF`（深蓝）
  - 中间值线性插值

**任务 6：实现可视化流水线**
- 文件：`src/visualization/visualization.py`
- 函数：`generate_visualization_sequence(results_dir: str, output_dir: str)`
- 功能：
  - 每隔 N 个 epoch 自动导出图
  - 生成 Gephi 预设文件（Force Atlas 2 布局）
  - 确保所有图布局一致，只有颜色变化

**任务 7：编写可视化分析 Notebook**
- 文件：`notebooks/visualization.ipynb`
- 内容：
  - 加载训练结果
  - 生成对比图（干预前后）
  - 制作论文 Figure 1

#### 4.3.3 代码命名规范
- **文件命名**：小写字母 + 下划线，如 `controller.py`、`gephi_exporter.py`
- **类命名**：大驼峰，如 `LLMController`、`ColorMapper`
- **函数命名**：小写字母 + 下划线，如 `get_intervention_weight`、`export_to_gexf`
- **变量命名**：小写字母 + 下划线，如 `subgraph_info`、`intervention_weight`
- **常量命名**：全大写 + 下划线，如 `DEFAULT_MODEL`、`COLOR_SCHEME`

---

## 五、核心接口对接

### 5.1 安迪 ↔ 哈工爷

**安迪提供：**
- `reward.py` 中的 `compute_reward(state, next_state)` 函数
- 状态表示格式：图对象（NetworkX Graph），节点有 `opinion` 属性

**哈工爷提供：**
- `OpinionDynamicsEnv` 类，实现 `reset()` 和 `step(action_node, intervention_weight)` 方法
- 环境初始化时接收 `reward_fn` 参数（安迪的奖励函数）

**对接示例：**
```python
# 安迪的代码
from src.environment.opinion_env import OpinionDynamicsEnv
from src.agent.reward import compute_reward

env = OpinionDynamicsEnv(graph, budget=K, reward_fn=compute_reward)
state = env.reset()
next_state, reward, done, info = env.step(action_node, intervention_weight)
```

### 5.2 安迪 ↔ 上交爷

**安迪调用：**
- `LLMController.get_intervention_weight(node_id, graph)` → 返回 0-1 的权重值
- `get_semantic_embedding(node_id)` → 返回语义嵌入向量（可选）

**上交爷提供：**
- `LLMController` 类，实现 `get_intervention_weight()` 方法
- 语义特征提取函数（可选）

**对接示例：**
```python
# 安迪的代码
from src.llm.controller import LLMController

llm_controller = LLMController()
intervention_weight = llm_controller.get_intervention_weight(node_id, graph)
```

### 5.3 上交爷 ↔ 哈工爷

**对接方式：**
- 上交爷生成的 `intervention_weight` 直接传给哈工爷的 `env.step()` 方法
- 无需额外接口，通过安迪的训练/测试脚本传递

---

## 六、数据流与工作流程

### 6.1 训练阶段（离线）

```
1. 哈工爷：生成合成图（30-50 节点）
   ↓
2. 安迪：初始化 Semantic-FINDER Agent
   ↓
3. 循环（每个 Episode）：
   a. 哈工爷：env.reset() → 初始状态
   b. 安迪：Agent.select_action(state) → 选择节点
   c. 上交爷（可选）：LLM.get_intervention_weight() → 权重（训练时可随机）
   d. 哈工爷：env.step(action, weight) → 下一状态 + 奖励
   e. 安迪：Agent.update(replay_buffer) → 更新参数
   ↓
4. 安迪：保存模型检查点
```

### 6.2 测试阶段（在线）

```
1. 哈工爷：加载真实数据集（SNAP，数千节点）
   ↓
2. 安迪：加载训练好的模型
   ↓
3. 循环（每个 Episode）：
   a. 哈工爷：env.reset() → 初始状态
   b. 安迪：Agent.select_action(state) → 选择节点
   c. 上交爷：LLM.get_intervention_weight(node, graph) → 权重
   d. 哈工爷：env.step(action, weight) → 下一状态 + 奖励
   e. 上交爷：可视化工具导出当前状态图
   ↓
4. 上交爷：生成对比图（干预前后）
```

---

## 七、配置文件规范

### 7.1 主配置文件（config/config.yaml）

```yaml
# 项目配置
project:
  name: "Semantic-FINDER"
  version: "1.0.0"

# 数据配置
data:
  synthetic:
    n_nodes: 50
    graph_type: "BA"
    n_episodes: 10000
  real:
    dataset_name: "ego-Twitter"
    data_path: "data/raw/"

# 环境配置
environment:
  budget: 10  # 干预预算 K
  dynamics_model: "degroot"  # "degroot" 或 "friedkin_johnsen"
  update_steps: 3
  alpha: 0.5  # Friedkin-Johnsen 参数

# Agent 配置
agent:
  encoder_type: "GraphSAGE"  # "GraphSAGE" 或 "GCN"
  hidden_dim: 128
  learning_rate: 0.001
  batch_size: 32
  gamma: 0.99  # 折扣因子
  epsilon_start: 1.0
  epsilon_end: 0.01
  epsilon_decay: 0.995
  replay_buffer_size: 10000

# LLM 配置
llm:
  model_name: "Qwen/Qwen-7B"
  device: "cuda"
  use_llm_in_training: false  # 训练时是否使用 LLM（建议 false）
  use_llm_in_testing: true    # 测试时是否使用 LLM

# 奖励函数配置
reward:
  method: "variance"  # "variance", "weighted_disagreement", "echo_chamber"

# 可视化配置
visualization:
  export_frequency: 10  # 每 N 个 epoch 导出一次
  output_dir: "results/figures/"
  color_scheme: "red_white_blue"

# 训练配置
training:
  max_episodes: 10000
  save_frequency: 100  # 每 N 个 episode 保存一次
  log_frequency: 10
  checkpoint_dir: "results/models/"

# 测试配置
testing:
  n_runs: 10  # 运行次数（取平均）
  output_dir: "results/test/"
```

---

## 八、依赖包管理

### 8.1 requirements.txt

```txt
# 核心框架
torch>=2.0.0
torch-geometric>=2.3.0
gymnasium>=0.28.0
numpy>=1.24.0
scipy>=1.10.0

# 图处理
networkx>=3.0
dgl>=1.1.0  # 可选，如果使用 DGL

# LLM
transformers>=4.30.0
vllm>=0.2.0  # 可选，用于高效推理
accelerate>=0.20.0

# 数据处理
pandas>=2.0.0
scikit-learn>=1.3.0

# 可视化
matplotlib>=3.7.0
seaborn>=0.12.0
plotly>=5.14.0

# 工具
tqdm>=4.65.0
pyyaml>=6.0
tensorboard>=2.13.0  # 可选，用于训练监控

# 开发工具
pytest>=7.3.0
black>=23.3.0
flake8>=6.0.0
```

---

## 九、代码提交规范

### 9.1 Git 提交信息格式

```
<类型>(<范围>): <主题>

<详细描述>

<相关 Issue>
```

**类型**：
- `feat`: 新功能（安迪、哈工爷、上交爷）
- `fix`: 修复 bug
- `docs`: 文档更新
- `refactor`: 代码重构
- `test`: 测试相关
- `chore`: 构建/工具相关

**范围**：
- `agent`: Agent 相关（安迪）
- `env`: 环境相关（哈工爷）
- `llm`: LLM 相关（上交爷）
- `viz`: 可视化相关（上交爷）
- `utils`: 工具函数

**示例**：
```
feat(agent): 实现 Semantic-FINDER 主类

- 实现 forward 方法计算 Q 值
- 实现 select_action 方法选择动作
- 添加 epsilon-greedy 探索策略

Related to #1
```

### 9.2 分支管理

- `main`: 主分支（稳定版本）
- `dev`: 开发分支
- `feature/andy-*`: 安迪的功能分支
- `feature/hit-*`: 哈工爷的功能分支
- `feature/sjtu-*`: 上交爷的功能分支

---

## 十、测试规范

### 10.1 单元测试

每个模块都需要编写单元测试：

- `tests/test_agent/`: 安迪的 Agent 测试
- `tests/test_environment/`: 哈工爷的环境测试
- `tests/test_llm/`: 上交爷的 LLM 测试
- `tests/test_visualization/`: 上交爷的可视化测试

### 10.2 集成测试

- `tests/integration/test_training_pipeline.py`: 测试完整训练流程
- `tests/integration/test_testing_pipeline.py`: 测试完整测试流程

---

## 十一、文档更新日志

| 日期 | 版本 | 更新内容 | 更新人 |
|------|------|----------|--------|
| 2024-01-07 | 1.0.0 | 初始版本，定义项目结构和接口规范 | 安迪 |

---

## 十二、联系方式与协作

### 12.1 代码审查

- 安迪负责审查所有代码
- 哈工爷和上交爷的代码提交后，需要安迪 review 后才能合并到主分支

### 12.2 问题反馈

- 使用 GitHub Issues 跟踪问题和任务
- 标签分类：
  - `bug`: Bug 报告
  - `feature`: 功能请求
  - `question`: 问题咨询
  - `documentation`: 文档相关

### 12.3 定期会议

- **周会时间**：每周一上午 10:00
- **会议内容**：
  1. 上周进度汇报
  2. 本周任务安排
  3. 技术问题讨论
  4. 接口对接确认

---

## 附录：关键参考资料

1. **FINDER 论文**: Fan, C., et al. (2020). "Finding key players in complex networks through deep reinforcement learning." *Nature Machine Intelligence*.
2. **DeGroot 模型**: DeGroot, M. H. (1974). "Reaching a consensus." *Journal of the American Statistical Association*.
3. **SNAP 数据集**: https://snap.stanford.edu/data/
4. **PyTorch Geometric 文档**: https://pytorch-geometric.readthedocs.io/
5. **Gymnasium 文档**: https://gymnasium.farama.org/
6. **vLLM 文档**: https://docs.vllm.ai/

---

**文档维护者**: 安迪  
**最后更新**: 2024-01-07

