# Semantic-FINDER 程序说明文档

本文档详细说明 Semantic-FINDER 系统的整体流程、数据输入输出、以及核心概念（Reward 和 Action）的定义。

---

## 一、系统概述

Semantic-FINDER 是一个基于强化学习的关键节点发现与干预系统，用于在社交网络中通过认知干预降低群体极化度。

**核心思想**：
- 使用强化学习（DQN）学习选择最优的干预节点
- 通过 LLM 生成干预策略（干预权重）
- 在环境中执行干预并观察效果
- 目标是最大化极化度的下降量

---

## 二、核心概念定义

### 2.1 Action（动作）

**定义**：Action 是 Agent 在每个时间步选择的动作，表示要干预哪个节点。

**具体形式**：
- **类型**：离散动作空间
- **值**：节点索引（整数），范围 [0, N-1]，其中 N 是图的节点数量
- **含义**：选择网络中的节点 $v_{target}$ 进行干预

**示例**：
```python
action = 5  # 表示选择节点 5 进行干预
```

**约束**：
- 已选过的节点不能重复选择（使用 Masking 机制）
- 每个 Episode 最多选择 `budget` 个节点（如 10 个）

**代码位置**：
- `src/agent/semantic_finder.py` 中的 `select_action()` 方法
- 使用 epsilon-greedy 策略：以 epsilon 概率随机选择（探索），以 (1-epsilon) 概率选择 Q 值最大的节点（利用）

---

### 2.2 Reward（奖励）

**定义**：Reward 是 Agent 执行动作后获得的即时奖励，表示该动作的好坏。

**计算公式**：
```
r_t = P(G_t) - P(G_{t+1})
```
其中：
- $P(G_t)$ 是当前时刻的极化度
- $P(G_{t+1})$ 是下一时刻的极化度
- 奖励 = 极化度下降量

**含义**：
- **正奖励**：极化度下降（好的行为，应该鼓励）
- **负奖励**：极化度上升（不好的行为，应该避免）
- **零奖励**：极化度不变

**三种极化度计算方法**：

#### 方法 1：方差（Variance）
```
P(G) = Var(x) = (1/N) * Σ(x_i - x̄)²
```
- 计算所有节点观点值的方差
- 方差越大，极化度越高
- 最简单的方法

#### 方法 2：加权分歧（Weighted Disagreement）
```
P(G) = Σ_{(i,j) ∈ E} w_ij * (x_i - x_j)²
```
- 计算所有边上的观点差异加权和
- 考虑图结构，邻居间差异越大，极化度越高
- 更符合社交网络的实际情况

#### 方法 3：回音室指数（Echo Chamber Score）
```
P(G) = 1 - (跨阵营边数 / 总边数)
```
- 计算不同阵营间的边数占比
- 跨阵营边越少，回音室效应越强，极化度越高
- 值在 0-1 之间

**代码位置**：
- `src/agent/reward.py`
- 函数：`compute_reward(state, next_state, method="variance")`

**示例**：
```python
from src.agent.reward import compute_reward

# 计算奖励（使用方差方法）
reward = compute_reward(current_graph, next_graph, method="variance")
print(f"奖励: {reward}")  # 正数表示极化度下降
```

---

### 2.3 State（状态）

**定义**：State 是环境的当前状态，表示网络的完整信息。

**具体形式**：
- **类型**：NetworkX Graph 对象
- **节点属性**：每个节点必须有 `opinion` 属性（float，范围 -1 到 1）
- **图结构**：邻接矩阵（通过 NetworkX 的边表示）

**状态表示**：
```python
import networkx as nx

# 状态是一个图对象
state = nx.Graph()
state.add_node(0, opinion=-0.8)
state.add_node(1, opinion=0.9)
state.add_edge(0, 1)

# 访问节点观点值
opinion_0 = state.nodes[0]['opinion']  # -0.8
```

**状态特征提取**：
Agent 从状态中提取特征：
- $d_v$: 归一化度数（结构信息）
- $x_v$: 观点值（-1 到 1）
- $e_v$: 语义嵌入（可选，由上交爷提供）

---

## 三、系统流程

### 3.1 训练流程（离线训练）

```
┌─────────────────────────────────────────────────────────────┐
│                    训练流程（Training）                       │
└─────────────────────────────────────────────────────────────┘

1. 初始化阶段
   ├─ 生成合成图（30-50 节点，Barabasi-Albert 网络）
   ├─ 初始化 Agent（Semantic-FINDER）
   ├─ 初始化环境（OpinionDynamicsEnv）
   └─ 初始化经验回放缓冲区（ReplayBuffer）

2. 主训练循环（每个 Episode）
   │
   ├─ Step 1: 环境重置
   │   └─ env.reset() → 返回初始状态图
   │
   ├─ Step 2: Agent 选择动作
   │   ├─ Agent 观察当前状态图
   │   ├─ 使用 GraphSAGE/GCN 编码图结构
   │   ├─ 使用 DQN 解码器计算 Q 值
   │   └─ 使用 epsilon-greedy 策略选择节点（Action）
   │
   ├─ Step 3: 获取干预权重（可选）
   │   ├─ 训练时：使用随机权重（加速训练）
   │   └─ 或调用 LLM Controller 生成权重
   │
   ├─ Step 4: 环境步进
   │   ├─ env.step(action, weight)
   │   ├─ 干预：修正选中节点的观点值
   │   ├─ 演化：运行观点动力学模型 3-5 轮
   │   └─ 计算奖励：reward = P(G_t) - P(G_{t+1})
   │
   ├─ Step 5: 存储经验
   │   └─ replay_buffer.push(state, action, reward, next_state, done)
   │
   └─ Step 6: 更新 Agent
       ├─ 从缓冲区采样一批经验
       ├─ 计算 Q 值损失
       └─ 反向传播更新网络参数

3. 定期保存
   └─ 每 N 个 Episode 保存模型检查点
```

**数据流**：
```
合成图 → 环境初始化 → 状态图
                              ↓
                         Agent 选择动作
                              ↓
                        获取干预权重
                              ↓
                        环境步进
                              ↓
                     (下一状态, 奖励, done)
                              ↓
                        存储到缓冲区
                              ↓
                        更新 Agent
```

---

### 3.2 测试流程（在线应用）

```
┌─────────────────────────────────────────────────────────────┐
│                    测试流程（Testing）                        │
└─────────────────────────────────────────────────────────────┘

1. 加载阶段
   ├─ 加载训练好的模型
   ├─ 加载真实数据集（SNAP，数千节点）
   └─ 初始化环境

2. 测试循环（每个 Episode）
   │
   ├─ Step 1: 环境重置
   │   └─ env.reset() → 返回初始状态图
   │
   ├─ Step 2: Agent 选择动作（不探索）
   │   └─ Agent.select_action(epsilon=0.0) → 选择最优节点
   │
   ├─ Step 3: LLM 生成干预权重
   │   └─ LLMController.get_intervention_weight(node, graph)
   │
   ├─ Step 4: 环境步进
   │   └─ env.step(action, weight) → 执行干预
   │
   └─ Step 5: 记录结果
       ├─ 记录极化度变化
       ├─ 记录选择的节点
       └─ 导出可视化图（可选）

3. 生成报告
   └─ 计算平均极化度下降、总奖励等指标
```

---

## 四、数据输入输出

### 4.1 训练阶段

#### 输入数据

**1. 合成图（由哈工爷生成）**
```python
# 输入：图生成参数
n_nodes = 50
graph_type = "BA"  # Barabasi-Albert

# 输出：NetworkX Graph 对象
graph = generate_synthetic_graph(n_nodes=50, graph_type="BA")
# 每个节点有 'opinion' 属性（-1 到 1）
```

**2. 训练参数**
```python
num_episodes = 10000      # 训练轮数
budget = 10               # 干预预算
learning_rate = 0.001     # 学习率
batch_size = 32           # 批次大小
gamma = 0.99              # 折扣因子
epsilon_start = 1.0       # 初始探索率
epsilon_end = 0.01        # 最终探索率
```

#### 输出数据

**1. 模型检查点**
```python
# 保存位置：results/models/
checkpoint_episode_100.pth
checkpoint_episode_200.pth
...
final_model.pth
```

**2. 训练日志**
- Episode 奖励
- Episode 损失
- 探索率变化

---

### 4.2 测试阶段

#### 输入数据

**1. 训练好的模型**
```python
model_path = "results/models/final_model.pth"
agent.load(model_path)
```

**2. 真实数据集（由哈工爷加载）**
```python
dataset_name = "ego-Twitter"
graph = load_graph(dataset_name, opinion_init="bimodal")
# 图可能有数千到数万节点
```

#### 输出数据

**1. 测试结果（JSON 格式）**
```json
{
  "dataset": "ego-Twitter",
  "n_runs": 10,
  "budget": 10,
  "avg_initial_polarization": 0.5234,
  "avg_final_polarization": 0.3121,
  "avg_polarization_reduction": 40.35,
  "avg_total_reward": 0.2113,
  "all_results": [...]
}
```

**2. 可视化图（可选，由上交爷生成）**
- Gephi 格式文件（.gexf）
- 展示干预前后的网络状态

---

## 五、关键数据转换

### 5.1 图对象 → PyTorch Geometric Data

**转换函数**：`graph_to_pyg_data()`

**输入**：
```python
graph = nx.Graph()
graph.add_node(0, opinion=-0.8)
graph.add_node(1, opinion=0.9)
graph.add_edge(0, 1)
```

**输出**：
```python
from torch_geometric.data import Data

data = Data(
    x=torch.tensor([
        [0.5, -0.8],  # [归一化度数, 观点值]
        [0.5, 0.9]
    ]),
    edge_index=torch.tensor([
        [0, 1, 1, 0],  # 边的索引
        [1, 0, 0, 1]
    ])
)
```

**代码位置**：`src/agent/encoder.py`

---

### 5.2 节点特征提取

**输入**：NetworkX Graph 对象

**处理**：
```python
for node in graph.nodes():
    # 特征1：归一化度数
    degree = graph.degree(node)
    max_degree = max(dict(graph.degree()).values())
    normalized_degree = degree / max_degree
    
    # 特征2：观点值
    opinion = graph.nodes[node]['opinion']
    
    # 特征向量：[d_v, x_v]
    features = [normalized_degree, opinion]
```

**输出**：特征矩阵 `[N, 2]`，其中 N 是节点数量

---

## 六、完整示例

### 6.1 训练示例

```python
from src.agent.semantic_finder import SemanticFINDER
from src.agent.reward import compute_reward
from src.environment.opinion_env import OpinionDynamicsEnv
from src.environment.data_loader import generate_synthetic_graph

# 1. 生成合成图
graph = generate_synthetic_graph(n_nodes=50, graph_type="BA")

# 2. 初始化 Agent
agent = SemanticFINDER(device="cuda")

# 3. 初始化环境
env = OpinionDynamicsEnv(
    graph=graph,
    budget=10,
    reward_fn=lambda s, ns: compute_reward(s, ns, method="variance")
)

# 4. 训练循环
for episode in range(100):
    state = env.reset()
    done = False
    
    while not done:
        # Agent 选择动作
        action = agent.select_action(state, epsilon=0.1)
        
        # 获取干预权重（训练时用随机值）
        intervention_weight = np.random.uniform(0.3, 0.7)
        
        # 环境步进
        next_state, reward, done, info = env.step(action, intervention_weight)
        
        # 存储经验
        replay_buffer.push(state, action, reward, next_state, done)
        
        # 更新 Agent
        if len(replay_buffer) > 32:
            agent.update(replay_buffer, batch_size=32)
        
        state = next_state
```

### 6.2 测试示例

```python
from src.llm.controller import LLMController

# 1. 加载模型
agent = SemanticFINDER(device="cuda")
agent.load("results/models/final_model.pth")

# 2. 加载真实数据
graph = load_graph("ego-Twitter")

# 3. 初始化环境
env = OpinionDynamicsEnv(graph=graph, budget=10, reward_fn=compute_reward)

# 4. 初始化 LLM
llm_controller = LLMController()

# 5. 测试循环
state = env.reset()
for step in range(10):
    # Agent 选择动作（不探索）
    action = agent.select_action(state, epsilon=0.0)
    
    # LLM 生成干预权重
    intervention_weight = llm_controller.get_intervention_weight(action, state)
    
    # 环境步进
    next_state, reward, done, info = env.step(action, intervention_weight)
    
    print(f"Step {step}: 选择节点 {action}, 奖励 {reward:.4f}")
    state = next_state
```

---

## 七、文件结构说明

```
src/agent/
├── semantic_finder.py    # 主 Agent 类
├── encoder.py            # 图编码器（GraphSAGE/GCN）
├── decoder.py            # Q 值解码器（DQN）
├── reward.py             # 奖励函数定义
└── replay_buffer.py      # 经验回放缓冲区

experiments/
├── train.py              # 训练脚本
└── test.py               # 测试脚本
```

---

## 八、关键参数说明

| 参数 | 含义 | 默认值 | 说明 |
|------|------|--------|------|
| `budget` | 干预预算 | 10 | 每个 Episode 最多选择的节点数 |
| `epsilon` | 探索率 | 1.0→0.01 | 探索 vs 利用的平衡 |
| `gamma` | 折扣因子 | 0.99 | 未来奖励的权重 |
| `learning_rate` | 学习率 | 0.001 | 模型更新步长 |
| `batch_size` | 批次大小 | 32 | 每次更新的经验数量 |
| `reward_method` | 奖励方法 | "variance" | 极化度计算方法 |

---

**文档维护者**: 安迪  
**最后更新**: 2024-01-07



