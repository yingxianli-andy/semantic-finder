\typeout{Semantic-FINDER Draft for IJCAI--ECAI 26}

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

\usepackage{ijcai26}

% Fonts and basic packages
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}

% Review submission: keep line numbers on; camera-ready: comment it out.
\linenumbers

\urlstyle{same}

% PDF Info Is REQUIRED (do NOT put Title/Author info here for blind review).
\pdfinfo{
/TemplateVersion (IJCAI.2026.0)
}

\title{Semantic-FINDER：面向社交网络极化缓解的语义增强关键节点发现与认知干预}

% Blind review: do not include author identities.
\author{Anonymous Authors}

\begin{document}
\maketitle

\begin{abstract}
本文提出 Semantic-FINDER：一个将观点动力学仿真、基于图神经网络的强化学习关键节点选择，以及大语言模型(LLM)的语义策略生成相结合的框架。与传统 ``Node Removal'' 的网络干预不同，我们关注在不封号/删帖等硬性手段的前提下，通过对少量关键节点进行认知干预，尽可能降低群体极化或抑制谣言传播。本文给出问题设定、方法框架、实验流程与图表规划的初稿，便于后续迭代完善与补齐实验细节。
\end{abstract}

\section{引言}
在大规模社交网络中，极化与谣言扩散会带来显著的社会风险。现实治理中常见的封号、删帖等强制措施会破坏网络连通性并可能引发反弹。我们关注一种更 ``软性'' 的治理思路：通过向少量节点推送特定信息(认知干预)来改变整体观点分布，使群体趋于融合，同时尽可能降低干预代价(干预节点数量/预算)。

传统影响力最大化方法通常将节点视为同质个体，忽略信息语义内容(即 ``说什么'')的重要性。另一方面，FINDER 证明了在小图上进行离线强化学习训练可泛化到大图的在线应用。本文的出发点是：将 FINDER 从 ``破坏网络结构'' 的节点移除任务扩展到 ``优化观点分布'' 的认知干预任务，并进一步引入 LLM 的语义理解能力，生成更合适的干预参数。

\section{问题定义与动机}
\subsection{论文要解决的问题}
给定一个大规模社交网络图 \(G=(V,E)\)，每个节点 \(i\in V\) 具有观点状态 \(x_i(t)\in[-1,1]\)，表示节点在时刻 \(t\) 的观点倾向。我们的目标是在预算约束 \(K\) 下，选择 \(K\) 个关键节点并对其施加认知干预，使得整体极化指标 \(P(t)\) 尽可能降低，同时避免对网络结构进行硬性破坏(如节点移除)。

具体而言，在每个时间步 \(t\)，智能体需要：
\begin{enumerate}
    \item \textbf{选择节点}：从 \(V\) 中选择一个节点 \(v_t\) 进行干预（已选节点不可重复选择）；
    \item \textbf{生成干预权重}：为选中的节点生成干预权重 \(\omega_t\in[0,1]\)，表示干预强度；
    \item \textbf{应用干预}：将干预作用于节点 \(v_t\)，并通过观点动力学模型更新整个网络的状态；
    \item \textbf{评估效果}：计算极化度下降量作为奖励信号，指导后续决策。
\end{enumerate}

该问题可形式化为一个马尔可夫决策过程(MDP)，其中状态空间为图的当前状态（包括节点观点值），动作空间为节点选择，奖励函数为极化度的下降量。

\subsection{动机}
\paragraph{现实痛点.}
封号/删帖等强制治理会破坏连通性并可能导致反弹；需要 ``软性'' 干预手段。在实际社交网络治理中，通过认知干预（推送特定信息）来改变观点分布，比硬性删除节点更加温和且有效。

\paragraph{技术瓶颈.}
传统影响力最大化方法通常将节点视为同质个体，忽略信息语义内容的重要性。在认知干预场景中，``对谁说什么''（即信息内容与投放方式）可能比 ``对谁说''（即节点选择）更关键。现有方法缺乏将语义理解与结构分析相结合的能力。

\paragraph{方法启发.}
FINDER 的 ``Offline Training, Online Application'' 思路为大规模网络上的关键节点发现提供了可泛化的强化学习范式。我们在此基础上引入 LLM 的语义理解能力，实现结构分析与语义策略生成的联合优化。

\section{方法：Semantic-FINDER}
\subsection{核心思路}
\paragraph{从 ``模拟每个人怎么说'' 到 ``模拟观点数值如何流动''.}
我们不再运行大量 LLM 个体代理来模拟发言，而是使用观点动力学模型进行高效的数值仿真；LLM 作为 ``超级调解员/策略生成器''，读取网络/子图状态的文本化描述并输出干预参数；强化学习(FINDER 风格)负责在每一步选择对哪些节点施加该策略。这种设计既保证了计算效率（避免大量 LLM 调用），又充分利用了 LLM 的语义理解能力。

\subsection{环境：观点动力学模拟}
我们以经典观点动力学为环境模型，支持 DeGroot 和 Friedkin--Johnsen(FJ) 两种模型。对于 DeGroot 模型，我们实现了支持干预项的新公式：

\begin{equation}
x_i(t+1) = \alpha \cdot x_i(t) + (1-\alpha) \cdot \left( \sum_{j\in \mathcal{N}(i)} w_{ij}\,x_j(t) + \text{Intervention}_i \right),
\label{eq:opinion-dynamics}
\end{equation}

其中 \(\alpha\in[0,1]\) 为顽固度参数（stubbornness），表示节点对当前观点的坚持程度；\(w_{ij}\) 为归一化权重，通常取 \(w_{ij}=1/|\mathcal{N}(i)|\)（即行归一化的邻接矩阵）；\(\text{Intervention}_i\) 为干预项，当节点 \(i\) 被选中时，由 LLM 生成的权重 \(\omega_i\) 决定其大小。

对于 FJ 模型，我们使用标准形式：
\begin{equation}
x_i(t+1) = \alpha \sum_{j\in \mathcal{N}(i)} w_{ij}\,x_j(t) + (1-\alpha)\,x_i(0),
\end{equation}
其中 \(x_i(0)\) 是初始意见锚定。

环境实现采用 Gymnasium 接口，支持 GPU 加速（使用 PyTorch 稀疏矩阵运算），可高效处理大规模网络（数万节点）。

\subsection{结构层：FINDER-based 关键节点选择}
我们使用基于图神经网络的编码器-解码器架构实现关键节点选择：

\textbf{编码器（Encoder）}：采用 GraphSAGE 或 GCN 作为图编码器，将图结构与节点特征编码为节点嵌入。节点特征包括：
\begin{itemize}
    \item 结构特征：归一化的节点度数 \(d_i\)
    \item 观点特征：当前观点值 \(x_i(t)\)
    \item 语义特征（可选）：由 LLM 预提取的语义嵌入 \(e_i\)
\end{itemize}

编码器输出每个节点的 \(d\)-维嵌入向量 \(\mathbf{h}_i\)，捕获节点的结构位置和状态信息。

\textbf{解码器（Decoder）}：采用深度 Q 网络(DQN)，输入节点嵌入 \(\mathbf{h}_i\)，输出 Q 值 \(Q(s, a_i)\)，表示选择节点 \(i\) 的期望累积奖励。解码器为多层感知机(MLP)，包含多个隐藏层和 Dropout 正则化。

\textbf{训练策略}：采用经验回放（Experience Replay）和 \(\epsilon\)-贪婪探索策略。在离线训练阶段，我们在小规模合成图（30-50 节点，Barabasi-Albert 无标度网络）上训练模型；训练完成后，直接应用到大规模真实网络，实现 ``Offline Training, Online Application'' 的泛化范式。

\subsection{语义层：LLM 干预参数生成}
当强化学习选择出候选关键节点 \(v_t\) 后，我们提取其局部子图信息，包括：
\begin{itemize}
    \item 节点自身观点值 \(x_{v_t}(t)\)
    \item 邻居节点数量及观点分布
    \item 邻居间观点差异（对立程度）
    \item 子图的紧密程度（聚类系数）
\end{itemize}

这些信息被文本化后输入 LLM（我们使用 Qwen2-7B-Instruct 模型），LLM 分析节点所在子图的语义特征（如 ``这是一个保守派聚集的紧密社群''），并输出干预权重 \(\omega_t\in[0,1]\)。该权重表示干预强度：\(\omega_t\) 接近 1 表示需要强力干预，接近 0 表示轻微干预即可。

干预权重通过以下方式作用于环境：在观点动力学更新时，被选中节点的观点值被调整为 \(x_{v_t}(t) \leftarrow x_{v_t}(t) \cdot (1-\omega_t)\)，即向中性观点（0）拉近，然后通过动力学模型传播到整个网络。

\subsection{优化目标与奖励设计}
我们定义了三种极化度计算方法，用于评估网络极化程度：

\textbf{方差方法（Variance）}：
\begin{equation}
P_{\text{var}}(t) = \frac{1}{|V|}\sum_{i\in V}\bigl(x_i(t)-\bar{x}(t)\bigr)^2,\quad
\bar{x}(t)=\frac{1}{|V|}\sum_{i\in V} x_i(t).
\label{eq:polarization-var}
\end{equation}

\textbf{加权分歧方法（Weighted Disagreement）}：
\begin{equation}
P_{\text{wd}}(t) = \sum_{(i,j)\in E} w_{ij} \bigl(x_i(t)-x_j(t)\bigr)^2,
\label{eq:polarization-wd}
\end{equation}
表示邻居间观点差异的加权和（Lyapunov 函数）。

\textbf{回音室指数（Echo Chamber Score）}：
\begin{equation}
P_{\text{ec}}(t) = 1 - \frac{|\{(i,j)\in E: \text{sign}(x_i(t)) \neq \text{sign}(x_j(t))\}|}{|E|},
\label{eq:polarization-ec}
\end{equation}
表示跨阵营边数占比，值越大表示回音室效应越强。

在一步干预后，奖励定义为极化度的下降量：
\begin{equation}
r_t = P(t-1)-P(t),
\label{eq:reward}
\end{equation}
其中 \(P(t)\) 可以是上述三种方法之一（默认使用方差方法）。奖励为正表示极化度下降（好的行为），奖励为负表示极化度上升（不好的行为）。

\section{实验设置与流程(对齐 FINDER 方法论)}
\subsection{阶段一：离线训练(小图)}
遵循 FINDER 的经验，不在大图上训练，而是在 \(30\sim 50\) 节点的合成图上训练强化学习策略。例如使用 NetworkX 生成 BA 无标度网络，随机初始化观点后进行若干步交互，更新 DQN 参数。

\subsection{阶段二：在线应用(大图泛化)}
将离线训练好的策略直接应用到真实大规模网络(如 SNAP 数据集或 Network Repository)。在在线阶段引入 LLM：当关键节点被选中时，对其局部结构与对立程度进行描述，喂给 LLM 生成干预参数，再反馈到模拟器中观察全局观点融合效果。

\subsection{硬件需求(初稿)}
由于不再运行大量 LLM 个体，计算压力主要在 RL 训练与图编码上。一个可行的起步配置如下表所示。

\begin{table}[t]
\centering
\begin{tabular}{lrr}
\toprule
配置 & GPU 显存(单卡) & 备注 \\
\midrule
2$\times$ RTX 4090 & 24GB & 足够进行 GNN+DQN 训练与 LLM 低频推理 \\
1$\times$ A100 & 80GB & 适合更大 batch 或更大模型/更频繁推理 \\
\bottomrule
\end{tabular}
\caption{硬件需求建议(初稿)。}
\label{tab:hardware}
\end{table}

\section{图表规划(占位)}
\subsection{Figure 1：Teaser}
\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figs/teaser.png}
\caption{Teaser：左为真实网络的红蓝对立(观点极化)，右为 Semantic-FINDER 干预后颜色混合(观点融合)。}
\label{fig:teaser}
\end{figure}

\subsection{Figure 2：方法框架}
\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figs/method.png}
\caption{方法框架示意：结构层(GNN+DQN)选择关键节点；语义层(LLM)生成干预参数；环境层(观点动力学)更新状态并返回奖励。}
\label{fig:method}
\end{figure}

\subsection{Figure 3：结果曲线}
\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figs/results.png}
\caption{结果曲线占位：横轴为预算(干预节点比例)，纵轴为极化指标 \(P(t)\) 或模块度等；对比随机、度中心性、PageRank、原始 FINDER(纯结构)与本文方法。}
\label{fig:results}
\end{figure}

\section{基线方法与对比(初稿)}
我们计划对比以下策略(后续可根据数据集与任务细化实现细节)：

\begin{table}[t]
\centering
\begin{tabular}{ll}
\toprule
方法 & 说明 \\
\midrule
Random & 随机选择干预节点(强基线) \\
Degree & 选择度最高的节点(传统 ``大V'' 策略) \\
PageRank & 选择 PageRank 高的节点 \\
FINDER (structure-only) & 仅用结构层策略，不使用 LLM 语义控制 \\
Semantic-FINDER (Ours) & 结构层选点 + LLM 语义生成干预参数 \\
\bottomrule
\end{tabular}
\caption{基线方法与本文方法(初稿)。}
\label{tab:baselines}
\end{table}

\section{结论与后续工作}
本文提出了 Semantic-FINDER 框架：以观点动力学作为可扩展环境，用 FINDER 风格的离线强化学习在结构层选择关键节点，并引入 LLM 在语义层输出干预参数，从而实现低成本的认知干预以缓解极化。框架实现了 ``Offline Training, Online Application'' 的范式，可在小规模合成图上训练，并泛化到大规模真实网络。

后续工作将包括：更严格的任务定义与评估指标、更系统的奖励与约束设计、在真实数据集（如 SNAP Twitter 数据集）上的完整实验、消融实验（验证 RL 和 LLM 模块的必要性）、鲁棒性测试（噪声和顽固度变化下的性能）、以及高质量可视化与图表。

\end{document}


